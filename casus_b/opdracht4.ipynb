{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique characters: 48\n",
      "Epoch 1/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 105ms/step - loss: 3.1386\n",
      "Epoch 2/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 110ms/step - loss: 3.0003\n",
      "Epoch 3/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 103ms/step - loss: 2.9884\n",
      "Epoch 4/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 107ms/step - loss: 2.9735\n",
      "Epoch 5/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 105ms/step - loss: 2.8853\n",
      "Epoch 6/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 128ms/step - loss: 2.8038\n",
      "Epoch 7/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 120ms/step - loss: 2.7452\n",
      "Epoch 8/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 126ms/step - loss: 2.6947\n",
      "Epoch 9/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 113ms/step - loss: 2.6236\n",
      "Epoch 10/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 115ms/step - loss: 2.5721\n",
      "Epoch 11/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 170ms/step - loss: 2.4991\n",
      "Epoch 12/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 165ms/step - loss: 2.4424\n",
      "Epoch 13/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 179ms/step - loss: 2.3826\n",
      "Epoch 14/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 173ms/step - loss: 2.2976\n",
      "Epoch 15/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 174ms/step - loss: 2.2482\n",
      "Epoch 16/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 175ms/step - loss: 2.2116\n",
      "Epoch 17/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - loss: 2.1361\n",
      "Epoch 18/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - loss: 2.0756\n",
      "Epoch 19/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - loss: 2.0319\n",
      "Epoch 20/20\n",
      "\u001b[1m231/231\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - loss: 1.9616\n",
      "kanker thet mrma cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane cane\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras.utils import to_categorical\n",
    "import random\n",
    "\n",
    "# Step 1: Load the data\n",
    "with open('data.txt', 'r') as file:\n",
    "    data = file.read().lower()  # Load and convert to lowercase\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Create a list of all unique characters in the data\n",
    "chars = sorted(list(set(data)))  # Sort to keep things consistent\n",
    "print(f\"Total unique characters: {len(chars)}\")\n",
    "\n",
    "# Create a character-to-index and index-to-character mapping\n",
    "char_to_index = {char: idx for idx, char in enumerate(chars)}\n",
    "index_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "# Step 3: Prepare the sequences (X and Y)\n",
    "sequence_length = 100  # Length of input sequences\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(0, len(data) - sequence_length):\n",
    "    seq_in = data[i:i + sequence_length]\n",
    "    seq_out = data[i + sequence_length]\n",
    "    X.append([char_to_index[char] for char in seq_in])\n",
    "    Y.append(char_to_index[seq_out])\n",
    "\n",
    "# Reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(X, (len(X), sequence_length, 1))\n",
    "\n",
    "# Normalize X by dividing by the total number of unique characters\n",
    "X = X / float(len(chars))\n",
    "\n",
    "# One-hot encode the labels Y\n",
    "Y = to_categorical(Y, num_classes=len(chars))\n",
    "\n",
    "# Step 4: Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X.shape[1], X.shape[2])))  # Input layer\n",
    "model.add(LSTM(128, return_sequences=True))  # LSTM layer 1\n",
    "model.add(LSTM(128))  # LSTM layer 2\n",
    "model.add(Dense(len(chars), activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Step 5: Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Step 6: Generate new sequence based on a seed\n",
    "def generate_sequence(seed, length=200):\n",
    "    # Start with the seed sequence\n",
    "    seed = seed.lower()  # Convert seed to lowercase\n",
    "    pattern = [char_to_index[char] for char in seed]\n",
    "    output = seed\n",
    "    \n",
    "    # Generate characters\n",
    "    for _ in range(length):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))  # Reshape for LSTM input\n",
    "        x = x / float(len(chars))  # Normalize input\n",
    "        predicted = model.predict(x, verbose=0)\n",
    "        \n",
    "        # Get the index of the predicted character\n",
    "        index = np.argmax(predicted)\n",
    "        result = index_to_char[index]\n",
    "        \n",
    "        # Append the predicted character to the output and update the pattern\n",
    "        output += result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]  # Keep the pattern the same length\n",
    "        \n",
    "    return output\n",
    "\n",
    "# Example: Generate a sequence starting with the seed 'kanker'\n",
    "generated_text = generate_sequence('kanker', 500)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelleren_van_kanker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
