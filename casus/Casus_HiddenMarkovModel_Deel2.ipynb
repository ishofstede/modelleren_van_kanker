{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96992fd3",
   "metadata": {},
   "source": [
    "# <span id=\"0\">Casus *Hidden Markov Model* - Deel II</span>\n",
    "\n",
    "Inhoud:\n",
    "\n",
    "* **<a href=\"#1\">Menti</a>**\n",
    "\n",
    "* **<a href=\"#2\">Kansrekening</a>**\n",
    "\n",
    "* **<a href=\"#3\">Experimentele waarschijnlijkheid</a>**\n",
    "\n",
    "* **<a href=\"#4\">Je eigen `HiddenMarkovModel` class</a>**\n",
    "\n",
    "* **<a href=\"#5\">CpG-eilandjes</a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "059cb5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac7499",
   "metadata": {},
   "source": [
    "<a id=\"1\" href=\"#0\" style=\"text-align: right; display: block;\">Terug naar boven</a>\n",
    "\n",
    "### Menti\n",
    "\n",
    "De onderstaande Menti gaat over kansrekening. We voeren deze uit tijdens de les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c5acd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='position: relative; padding-bottom: 56.25%; padding-top: 35px; height: 0; overflow: hidden;'><iframe sandbox='allow-scripts allow-same-origin allow-presentation' allowfullscreen='true' allowtransparency='true' frameborder='0' height='315' src='https://www.mentimeter.com/app/presentation/alao6y5iwixjzjyycjfe8hcfqk7jy199/embed' style='position: absolute; top: 0; left: 0; width: 100%; height: 100%;' width='420'></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div style='position: relative; padding-bottom: 56.25%; padding-top: 35px; height: 0; overflow: hidden;'><iframe sandbox='allow-scripts allow-same-origin allow-presentation' allowfullscreen='true' allowtransparency='true' frameborder='0' height='315' src='https://www.mentimeter.com/app/presentation/alao6y5iwixjzjyycjfe8hcfqk7jy199/embed' style='position: absolute; top: 0; left: 0; width: 100%; height: 100%;' width='420'></iframe></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c43f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UITWERKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645b677",
   "metadata": {},
   "source": [
    "<a id=\"2\" href=\"#0\" style=\"text-align: right; display: block;\">Terug naar boven</a>\n",
    "\n",
    "### Kansrekening\n",
    "\n",
    "Kansrekening is een tak van de wiskunde die zich richt op het begrijpen en kwantificeren van onzekerheid en willekeurige verschijnselen. Stel je voor dat je een dobbelsteen gooit: de uitkomst is onzeker en kan elk van de zes zijden laten zien. Kansrekening helpt ons om de waarschijnlijkheid van deze mogelijke uitkomsten te beschrijven.\n",
    "\n",
    "Een basisconcept in kansrekening is dat van een experiment en de mogelijke uitkomsten ervan. In ons voorbeeld is het gooien van een dobbelsteen het experiment, en de mogelijke uitkomsten zijn de getallen 1 tot en met 6. Een gebeurtenis is een specifieke set van uitkomsten die ons interesseert, zoals het gooien van een even getal.\n",
    "\n",
    "De kans van een gebeurtenis is een getal tussen 0 en 1 dat aangeeft hoe waarschijnlijk het is dat de gebeurtenis plaatsvindt. Een kans van 0 betekent dat de gebeurtenis nooit zal gebeuren, terwijl een kans van 1 betekent dat het altijd gebeurt. Bijvoorbeeld, bij een eerlijke dobbelsteen is de kans om een drie te gooien 1 op 6, oftewel $P(⚂) = \\frac{1}{6}$, omdat er zes mogelijke uitkomsten zijn die allemaal even waarschijnlijk zijn om op te treden, en slechts één daarvan is een drie. De kans om een even getal te gooien met een dobbelsteen is $P(\\text{even}) = \\frac{3}{6} = \\frac{1}{2}$ omdat drie van de zes mogelijke uitkomsten even zijn.\n",
    "\n",
    "Kansrekening omvat ook verschillende regels en concepten om complexere situaties te begrijpen. Zo zijn er regels die ons vertellen hoe we de kans berekenen dat meerdere gebeurtenissen plaatsvinden.\n",
    "\n",
    "Bijvoorbeeld, als je de kans wil weten dat je bij het gooien van een dobbelsteen een één of een twéé gooit, tel je de afzonderlijke kansen bij elkaar op:\n",
    "\n",
    "$$\n",
    "P(⚀ \\cup ⚁) = P(⚀) + P(⚁)\n",
    "$$\n",
    "\n",
    "Het $\\cup$-symbool staat voor \"of\" (de vereniging van twee verzamelingen). Deze regel gaat alleen op als het kansen zijn op gebeurtenissen die nooit of te nimmer allebei samen kunnen optreden: als je een één gooit, gooi je nooit tegelijkertijd een twéé.\n",
    "\n",
    "Daarentegen, als je de kans wil weten dat je bij het gooien van een dobbelsteen eerst een één en daarna met een nieuwe worp een twéé gooit, vermenigvuldig je de afzonderlijke kansen:\n",
    "\n",
    "$$\n",
    "P(⚀ \\cap ⚁) = P(⚀) \\cdot P(⚁)\n",
    "$$\n",
    "\n",
    "Het $\\cap$-symbool staat voor \"en\" (de doorsnede van twee verzamelingen). Deze regel gaat alleen op als het kansen zijn die onafhankelijk van elkaar optreden en niet van elkaar afhangen: het resultaat van een eerste worp zegt helemaal niets over dat van een tweede worp.\n",
    " \n",
    "Een ander belangrijk concept is de voorwaardelijke kans, die ons helpt te begrijpen hoe de kans op een gebeurtenis verandert als we al iets weten over een andere gebeurtenis. Stel je voor dat je een kaart trekt uit een standaard kaartspel, en je weet dat de kaart zwart is. De kans dat deze kaart een schoppen twee is, is anders dan de oorspronkelijke kans voordat je wist dat de kaart zwart was.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(♠2) = \\frac{1}{52}\n",
    "\\\\\n",
    "P(♠2 | \\text{zwart}) = \\frac{1}{26}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Het |-symbool staat voor \"gegeven dat\". Dit wordt ook wel een *voorwaardelijke* kans genoemd.\n",
    "\n",
    "Voorwaardelijke kansen zijn nuttig als je gebeurtenissen wil combineren die wél met elkaar samenhangen. Stel bijvoorbeeld dat ik de kans wil weten dat ik in één worp van een dobbelsteen een aantal ogen gooi dat zowel even is als groter dan drie. Je kan voor dit eenvoudige voorbeeld nagaan dat er twee uitkomsten even én groter dan drie zijn (⚃ en ⚅), dus die kans is\n",
    "\n",
    "$$\n",
    "P(\\text{ even } \\cap \\text{ >3 }) = \\frac{2}{6} = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "Echter, de kans dat een worp even is is $P(\\text{ even }) = \\frac{3}{6} = \\frac{1}{2}$ en de kans dat een uitkomst groter dan drie is is $P(\\text{ >3 }) = \\frac{3}{6} = \\frac{1}{2}$, dus in dit geval is dit niet gelijk aan het product van de losse kansen! Dit komt omdat de kansen niet onafhankelijk zijn: als je even gooit is de kans groter dat je groter dan drie gooit dan dat je minder gooit.\n",
    "\n",
    "Je kan kansen echter wel combineren met de productregel als je voorwaardelijke kansen gebruikt: $P(A \\cap B) = P(A) \\cdot P(B|A)$, of ook $P(A \\cap B) = P(B) \\cdot P(A|B)$. Zo is hier $P(\\text{ >3 } | \\text{ even }) = \\frac{2}{3}$. Ga zelf na hoe groot $P(\\text{ even } | \\text{ >3 })$ is.\n",
    "\n",
    "Met deze basisprincipes kunnen we complexere modellen doorrekenen, zoals Hidden Markov Modellen. Een Hidden Markov Model (HMM) maakt gebruik van kansrekening om sequenties van observaties te analyseren en te voorspellen. Het model gaat uit van een reeks verborgen toestanden die niet direct zichtbaar zijn, en een reeks waargenomen uitkomsten die afhankelijk zijn van deze toestanden. HMM's gebruiken transitiewaarschijnlijkheden om te berekenen hoe groot de kans is dat het systeem van de ene verborgen toestand naar de andere overgaat, en emissiekansen om te bepalen hoe waarschijnlijk een bepaalde waarneming is gegeven een specifieke verborgen toestand. Door deze kansen te combineren, kan een HMM bepalen hoe waarschijnlijk een gegeven reeks observaties is, of de meest waarschijnlijke reeks verborgen toestanden achter de reeks observaties bepalen.\n",
    "\n",
    "[Khan Academy](https://www.khanacademy.org/math/math2/xe2ae2386aa2e13d6:prob) presenteert een multimediale les over basisbegrippen in de kansrekening (**Unit 13: Probability**). Doorloop zelfstandig deze les, bekijk voor zover je dat nodig acht de video's, vul de oefeningen in, en doe de quizes. Je dient de afsluitende [Unit test](https://www.khanacademy.org/math/math2/xe2ae2386aa2e13d6:prob/xe2ae2386aa2e13d6:expected-value/test/xe2ae2386aa2e13d6:prob-unit-test?referrer=upsell) met succes af te ronden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7e6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UITWERKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93fc18",
   "metadata": {},
   "source": [
    "<a id=\"3\" href=\"#0\" style=\"text-align: right; display: block;\">Terug naar boven</a>\n",
    "\n",
    "### Experimentele waarschijnlijkheid\n",
    "\n",
    "De vorige les heb je een reeks knikkers getrokken aan drie verschillende tafels. De gegevens omtrent overgangswaarschijnlijkheden en emissiekansen kun je aflezen uit de bekende gegevens omtrent de opzet van het experiment:\n",
    "\n",
    "| Tafel: |  ❶  |  ❷  |  ❸  |\n",
    "| -----: | :-: | :-: | :-: |\n",
    "| **Grabbelton:** | 6x blauw | 2x blauw | 1x blauw |\n",
    "|                 | 3x geel  | 6x geel  | 0x geel  | \n",
    "|                 | 1x groen | 2x groen | 6x groen |\n",
    "|                 | 2x rood  | 2x rood  | 5x rood  |\n",
    "| **Dobbelsteen:** | ⚀→① | ⚀→① | ⚀→① |\n",
    "|                  | ⚁→② | ⚁→② | ⚁→① |\n",
    "|                  | ⚂→② | ⚂→② | ⚂→① |\n",
    "|                  | ⚃→② | ⚃→③ | ⚃→① |\n",
    "|                  | ⚄→③ | ⚄→③ | ⚄→② |\n",
    "|                  | ⚅→③ | ⚅→③ | ⚅→③ |\n",
    "\n",
    "Laten we eens proberen te berekenen wat de kans is op de volgende reeks van vijf uitkomsten:\n",
    "\n",
    "| **Beurt:** | 1     | 2     | 3     | 4     | 5     |\n",
    "| ---------: | :---: | :---: | :---: | :---: | :---: |\n",
    "| **Tafel:** | ❷     | ❸     | ❶     | ❸     | ❷     |\n",
    "| **Kleur:** | geel  | groen | blauw | rood  | groen |\n",
    "\n",
    "Merk op dat een Hidden Markov Model aanneemt dat we de toestanden (de tafels) niet kunnen waarnemen. In dit geval kennen we de tafels echter wel. We berekenen daarom voorlopig de kans op al deze kleuren én al deze tafels samen.\n",
    "\n",
    "##### Tafel ❷ in beurt 1\n",
    "\n",
    "Je bent aan een willekeurige tafel begonnen, dus de kans op elke tafel in beurt 1 was aanvankelijk gelijk. Dat betekent dat de kans dat je in beurt 1 aan tafel ❷ zou belanden gelijk is aan\n",
    "\n",
    "$$\n",
    "P(❷_1) = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "Die kans hangt verder nergens van af. Hierboven wordt een onderschrift gebruikt om het nummer van de beurt aan te geven.\n",
    "\n",
    "##### Kleur geel in beurt 1\n",
    "\n",
    "Gegeven nu dat je in beurt 1 aan tafel ❷ zat, kon je 6 verschillende gele knikkers trekken uit een totaal van 12 knikkers. Daarom is\n",
    "\n",
    "$$\n",
    "P(\\text{geel}_1|❷_1) = \\frac{6}{12}\n",
    "$$\n",
    "\n",
    "Deze kans hangt alléén af van het feit dat je in deze beurt aan tafel ❷ zat, en de kans dat dat aan de hand is hadden we hiervoor al berekend.\n",
    "\n",
    "##### Tafel ❸ in beurt 2\n",
    "\n",
    "Gegeven dat je in beurt 1 aan tafel ❷ zat, waren er drie uitkomsten van de dobbelsteen die je voor beurt 2 naar tafel ❸ zouden sturen. Daarom is\n",
    "\n",
    "$$\n",
    "P(❸_2|❷_1) = \\frac{3}{6}\n",
    "$$\n",
    "\n",
    "Ook deze kans hangt alléén af van het feit dat je hiervoor aan tafel ❷ zat, en dat hebben we hierboven al doorgerekend. Het hangt bijvoorbeeld niet af van het feit dat je hiervoor een gele knikker getrokken had.\n",
    "\n",
    "##### Kleur groen in beurt 2\n",
    "\n",
    "Gegeven dat je aan tafel ❸ zat, kon je wederom 6 groene knikkers trekken uit een totaal van 12 knikkers. Daarom is\n",
    "\n",
    "$$\n",
    "P(\\text{groen}_2|❸_2) = \\frac{6}{12}\n",
    "$$\n",
    "\n",
    "Deze kans hangt wederom alléén af van het feit dat je in deze beurt aan tafel ❸ zat, en die kans werd in de stap hiervoor berekend. De tafel of de kleur van de vorige beurt heeft geen rechtstreeks effect op deze uitkomst.\n",
    "\n",
    "##### Enzovoorts\n",
    "\n",
    "Voor de beurten daarna kunnen we op soortgelijke manier de kansen berekenen. Dankzij de aannamen van een Hidden Markov Model hangt de kans op een tafel in een beurt alléén af van de tafel in de vorige beurt, en de kleur van een knikker in een beurt alléén van de tafel in diezelfde beurt. We krijgen dan\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(❶_3|❸_2) &= \\frac{4}{6}\n",
    "\\\\\n",
    "P(\\text{blauw}_3|❶_3) &= \\frac{6}{12}\n",
    "\\\\\n",
    "P(❸_4|❶_3) &= \\frac{2}{6}\n",
    "\\\\\n",
    "P(\\text{rood}_4|❸_4) &= \\frac{5}{12}\n",
    "\\\\\n",
    "P(❷_5|❸_4) &= \\frac{1}{6}\n",
    "\\\\\n",
    "P(\\text{groen}_5|❷_5) &= \\frac{2}{12}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "##### Tezamen\n",
    "\n",
    "We kunnen nu de kansen hierboven gebruiken om te berekenen hoe groot de kans op de verkregen tafels en kleuren uit alle vijf de beurten samen is geweest. We zijn dan eigenlijk op zoek naar\n",
    "\n",
    "$$\n",
    "p = P(❷_1 \\cap \\text{geel}_1 \\cap ❸_2 \\cap \\text{groen}_2 \\cap ❶_3 \\cap \\text{blauw}_3 \\cap ❸_4 \\cap \\text{rood}_4 \\cap ❷_5 \\cap \\text{groen}_5 )\n",
    "$$\n",
    "\n",
    "Deze kansen zijn niet allemaal onafhankelijk van elkaar, maar we kunnen ze wel zo schrijven dat elke kans alleen afhangt van de gebeurtenissen eerder in de reeks. Daarom mogen we dit schrijven als een product\n",
    "\n",
    "$$\n",
    "p = P(❷_1) \\cdot P(\\text{geel}_1|❷_1) \\cdot P(❸_2|❷_1) \\cdot P(\\text{groen}_2|❸_2) \\cdot P(❶_3|❸_2) \\cdot P(\\text{blauw}_3|❶_3) \\cdot P(❸_4|❶_3) \\cdot P(\\text{rood}_4|❸_4) \\cdot P(❷_5|❸_4) \\cdot P(\\text{groen}_5|❷_5)\n",
    "$$\n",
    "\n",
    "En dit kunnen we nu uitrekenen als\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{3} \\cdot \\frac{6}{12} \\cdot \\frac{3}{6} \\cdot \\frac{6}{12} \\cdot \\frac{4}{6} \\cdot \\frac{6}{12} \\cdot \\frac{2}{6} \\cdot \\frac{5}{12} \\cdot \\frac{1}{6} \\cdot \\frac{2}{12} = \\frac{51840}{967458816} = \\frac{5}{93312} \\approx 0.000054\n",
    "$$\n",
    "\n",
    "Deze kans is erg klein, wat op zich niet verbazingwekkend is omdat de kans op elke specifieke reeks waarnemingen van tafels en kleuren niet erg groot kan zijn. Er zijn immers enorm veel verschillende reeksen tafels en kleuren die waargenomen hadden kunnen worden (maar niet elke reeks is even aannemelijk).\n",
    "\n",
    "Hoe langer de reeks waarnemingen, hoe kleiner deze kans wordt. Op den duur kan de kans zo ver afnemen dat floating-point waarden in een computer niet meer in staat zijn om deze minieme kansen goed te representeren: ze worden dan afgerond naar nul. Om dat te voorkomen wordt soms gewerkt met de logaritme van de kans: de *log-waarschijnlijkheid*. De rekenregels van logaritmen zeggen dat de logaritme van een product van getallen kan worden geschreven als de som van logaritmen van die getallen. De formule hierboven wordt dan\n",
    "\n",
    "$$\n",
    "\\ln \\left( p \\right) = \\ln \\left( P(\\text{geel}_1|❷_1) \\right) + \\ln \\left( P(\\text{geel}_1|❷_1) \\right) + \\ldots + \\ln \\left( P(\\text{groen}_5|❷_5) \\right)\n",
    "$$\n",
    "\n",
    "In plaats van waarschijnlijkheden te vermenigvuldigen kunnen we dus de bijbehorende log-waarschijnlijkheden simpelweg optellen. De uitkomsten worden hierbij niet zo extreem. Dit komt uiteindelijk neer op\n",
    "\n",
    "$$\n",
    "\\ln \\left( p \\right) \\approx -9.834\n",
    "$$\n",
    "\n",
    "Bereken nu voor de eerste vijf beurten uit jouw eigen experimentele reeks de waarschijnlijkheid $p$ en de log-waarschijnlijkheid $\\ln \\left( p \\right)$. Wissel je reeks uit met je groepsgenoot en controleer elkaars resultaten. Hebben jullie allebei een (ongeveer) even waarschijnlijke reeks waarnemingen verkregen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150d456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UITWERKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4360be02",
   "metadata": {},
   "source": [
    "<a id=\"4\" href=\"#0\" style=\"text-align: right; display: block;\">Terug naar boven</a>\n",
    "\n",
    "### Je eigen `HiddenMarkovModel` class\n",
    "\n",
    "De vorige les ben je begonnen met het aanmaken van een eigen klasse `HiddenMarkovModel`. Voeg nu een methode `score()` toe die twee argumenten ontvangt: een iterable met emissies (hier bijvoorbeeld de reeks kleuren; `X`, als we `hmmlearn` naamgeving volgen) en een iterable met toestanden (hier bijvoorbeeld de reeks tafels; `state_sequence`, volgens `hmmlearn`). Iterables zijn datastructuren waar je met een for-loop doorheen kan itereren; bijvoorbeeld python lijsten, tuples, of numpy-arrays. Je mag aannemen dat de emissies en toestanden gecodeerd zijn als python integers (bijvoorbeeld de kleuren blauw/geel/groen/rood als `0, 1, 2, 3` en de tafels ❶/❷/❸ als `0, 1, 2`). De methode dient op grond van de bekende begintoestandverdeling, emissiekansen, en overgangswaarschijnlijkheden te berekenen hoe groot de log-waarschijnlijkheid is op die reeks waarnemingen, en dat als retourwaarde teruggeven aan de gebruiker.\n",
    "\n",
    "Gebruik je eigen module om de berekening uit de voorgaande oefening te controleren. Komt er hetzelfde antwoord uit? Bereken ook de log-waarschijnlijkheid voor je hele reeks eigen waarnemingen, en voor de hele reeks waarnemingen van de hele klas; deze berekeningen zijn wat omslachtig om met de hand te doen, maar je klasse zou het met gemak aan moeten kunnen.\n",
    "\n",
    "De `hmmlearn` module definieert een soortgelijke `score()` methode die weliswaar de emissies `X` ontvangt, maar niet de toestanden `state_sequence`. Toestanden worden in een Hidden Markov Model eigenlijk niet waarneembaar verondersteld. We kunnen echter met onze eigen module hetzelfde resultaat bereiken als met `hmmlearn` door de kansen te sommeren over *alle mogelijke* reeksen toestanden. Als je Hidden Markov Model geïnitialiseerd is in de variabele `model` krijg je iets als het volgende:\n",
    "\n",
    "```python\n",
    "from itertools import product\n",
    "from math import exp, log\n",
    "\n",
    "prob_sum = 0.0\n",
    "for state_sequence in product(range(3), repeat=5):   # 5 herhalingen van 3 mogelijke toestanden\n",
    "    prob_sum += exp(model.score(X, state_sequence))\n",
    "log_prob = log(prob_sum)\n",
    "```\n",
    "\n",
    "Het resultaat hiervan zou dan hetzelfde moeten opleveren (wellicht op afrondfoutjes na) als een aanroep zoals `CategoricalHMM(...).score(X)` uit de `hmmlearn` module. Controleer dat de uitkomsten van je eigen module hiermee overeenkomen. Lukt het om de scores voor je eigen eerste vijf waarnemingen te bepalen, je volledige eigen reeks waarnemingen, of de reeks waarnemingen van de hele klas? Merk op dat de bovenstaande for-loop al snel enorm traag kan worden zodra het aantal waarnemingen toeneemt; we gaan dit gedrag in een latere les verbeteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51852e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States    : [1 2 0 2 1]\n",
      "Emissions : [1 2 0 3 2]\n",
      "Eigen module:   ln(p) = -9.834   (p = 5.4e-05)\n",
      "Dit komt overeen met de handmatige berekening!\n",
      "\n",
      "Som over ALLE mogelijke States:\n",
      "Eigen module:   ln(p) = -6.578   (p = 1.4e-03)\n",
      "hmmlearn:       ln(p) = -6.578   (p = 1.4e-03)\n",
      "Het resultaat van de eigen module komt overeen met dat van hmmlearn!\n"
     ]
    }
   ],
   "source": [
    "# UITWERKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16396dc2",
   "metadata": {},
   "source": [
    "<a id=\"5\" href=\"#0\" style=\"text-align: right; display: block;\">Terug naar boven</a>\n",
    "\n",
    "### CpG-eilandjes\n",
    "\n",
    "In het vorige deel heb je aan de hand van realistische transitiewaarschijnlijkheden willekeurige sequenties gegenereerd van 300 nucleotiden lang die overeenkomen met een lokatie in of buiten een CpG-eilandje. Neem die sequenties hier over, of genereer ze opnieuw.\n",
    "\n",
    "Instantieer ook twee Hidden Markov Modellen die elk vier toestanden hebben: A, C, T, en G. De overgangswaarschijnlijkheden kun je nu kiezen voor het ene model zoals ze zijn binnen een CpG-eilandje en voor het andere model zoals ze zijn erbuiten (zie de tabellen in deel I.3 *CpG-eiland en non-CpG-eiland sequenties*). In dit geval komen de toestanden exact overeen met de waarnemingen, dus de emissiekansen kunnen we als volgt instellen.\n",
    "\n",
    "|  `+`  |   A   |   C   |   G   |   T   |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| **A** |  1.0  |  0.0  |  0.0  |  0.0  |\n",
    "| **C** |  0.0  |  1.0  |  0.0  |  0.0  |\n",
    "| **T** |  0.0  |  0.0  |  1.0  |  0.0  |\n",
    "| **G** |  0.0  |  0.0  |  0.0  |  1.0  |\n",
    "\n",
    "De toestanden staan in de rijen, en de emissies in de kolommen. Deze tabel geeft aan dat toestand **A** altijd een **A** als emissie geeft, etc. Eigenlijk is dit niet zozeer een Hidden Markov Model, maar een [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain). Immers, de toestanden zijn niet onbekend (\"hidden\"), maar komen precies overeen met de waargenomen emissies. Echter, een Hidden Markov model is algemener dan een Markov Chain en kan ook hiermee omgaan.\n",
    "\n",
    "Je hebt nu dus een Hidden Markov Model dat de kans kan bepalen op een bepaalde sequentie als die binnen een CpG-eilandje voorkomt, en een ander model dat de kans bepaalt op de sequentie buiten een CpG-eilandje.\n",
    "\n",
    "Pas nu beide modellen toe op beide gegenereerde random sequenties. Je krijgt dus vier uitkomsten. Geeft het model dat past binnen een CpG-eilandje ook de hoogste kans aan de sequentie die past binnen een CpG-eilandje? En omgekeerd, geeft het model dat past buiten een CpG-eilandje ook de hoogste kans aan de sequentie die past buiten een CpG-eilandje?\n",
    "\n",
    "Als uitdaging, kun je de eigenschappen van beide Hidden Markov Modellen combineren in één model? Dat wil zeggen, kun je een model maken dat zowel sequenties binnen en buiten een CpG-eilandje kan verwerken en alle log-waarschijnlijkheden die je hierboven berekende kan evalueren? Waarom krijg je niet per se exact hetzelfde antwoord? (Dit model hoeft niet om te kunnen gaan met sequenties waarvan sommige delen binnen en andere delen buiten CpG-eilandjes liggen.) Hint: maak een model met acht verschillende toestanden en vier verschillende emissies.\n",
    "\n",
    "Tenslotte, gegeven is de volgende sequentie:\n",
    "\n",
    "```\n",
    "TCCCCGCAGGCCATAGCCCGGGACGTCCGACAGCCGGCTGGTGCTGGGGGTAGGCATAATCGCGAGAGCCACCGTCGTCTGTCTGCCTGCTGAGCCTTAG\n",
    "```\n",
    "\n",
    "Komt deze uit een CpG-eilandje, of van ergens daarbuiten? Kun je iets zinnigs zeggen over hoe zeker je bent van dit antwoord?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d7e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGI+ Sequence:\n",
      "GGACACGCTATGGGAGGGGGGGCCCCAACCCCCGCCGCCCGCTCCTGGACTGCGCCTCCCCCAGGGGCCCGAGAAAAGGGTGCCGGGCGCAGCATGCGCTTGGAAGCCGCGCGGTGTTGATGCCCCCGGACGCCCACAGGGTCGCGACGCGCTTCTACCTCTCCAGACTCTTCTGGACACTCCTGGAAGGGACACCAGGTGTGCCGAGCCTCCCCCTGCGGCCACGTCGCTGGGGCGCCCTGGCCGCGGCAGCCCCCCTGGCAGTGTTCGCCCTAGGAATGTCCCAAGCGTACCTCGGCC\n",
      "\n",
      "CGI- sequence:\n",
      "AAGTGGGCCTTCCCTTTGAAAGCTGCGTAGACACCCCATGGATGTAGGAGTTAAAATTCTGCACAACCTAAATCTAATGCGTGGAAGAAGTTCCTGGGATTTTGGGGAAAAGGCCAGGAATGCTATTTTGCACATCTGCCTAGGACCTGGCTTTGAACCAAGGGATAAATTCCTTTGATAGTGAATTAGTGGTAGTCAGAAAGATCAACCAAGCCTCATGCCAGCACATGGGCCCAGCCGTCTGACTTAATCATCTCAATGGAAACAAAGCTCACAGAGAGTGAACCACATGGAGCCTAA\n",
      "\n",
      "CGI+ seq volgens CGI+ model:\tln(p) = -383.374\n",
      "CGI- seq volgens CGI+ model:\tln(p) = -443.261\n",
      "CGI+ seq volgens CGI- model:\tln(p) = -426.543\n",
      "CGI- seq volgens CGI- model:\tln(p) = -400.798\n",
      "\n",
      "Volgens gecombineerd model:\n",
      "CGI+ seq in CGI+ gebied:\tln(p) = -384.067\n",
      "CGI- seq in CGI+ gebied:\tln(p) = -443.955\n",
      "CGI+ seq in CGI- gebied:\tln(p) = -427.236\n",
      "CGI- seq in CGI- gebied:\tln(p) = -401.491\n"
     ]
    }
   ],
   "source": [
    "# UITWERKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800564d3",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "&copy; 2024 - Dave R.M. Langers <d.r.m.langers@pl.hanze.nl>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
